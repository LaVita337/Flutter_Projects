{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./anaconda3/envs/test1020/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "/home/i15project23/data/coco/output_1.pt\n",
      "/home/i15project23/data/coco/output_3.pt\n",
      "/home/i15project23/data/coco/output_0.pt\n",
      "/home/i15project23/data/coco/output_2.pt\n",
      "/home/i15project23/data/coco/oscar_split_train_tokens.pkl\n",
      "/home/i15project23/data/coco/oscar_split_train.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "!pip install pandas\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/home/i15project23/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oscar_split_train.pkl\t      output_0.pt  output_2.pt\n",
      "oscar_split_train_tokens.pkl  output_1.pt  output_3.pt\n",
      "/home/i15project23/anaconda3/envs/test1020/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!ls /home/i15project23/data/coco\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from transformers) (23.2)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/test1020/lib/python3.7/site-packages (from requests->transformers) (2023.11.17)\n",
      "Installing collected packages: tokenizers, urllib3, tqdm, safetensors, regex, pyyaml, fsspec, filelock, charset-normalizer, requests, huggingface-hub, transformers\n",
      "Successfully installed charset-normalizer-3.3.2 filelock-3.12.2 fsspec-2023.1.0 huggingface-hub-0.16.4 pyyaml-6.0.1 regex-2023.10.3 requests-2.31.0 safetensors-0.4.0 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.30.2 urllib3-2.0.7\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 616435\n",
      "Train both prefix and GPT\n",
      ">>> Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i15project23/anaconda3/envs/test1020/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:07<00:00,  5.12it/s, loss=2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:22<00:00,  5.10it/s, loss=2.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:21<00:00,  5.10it/s, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:23<00:00,  5.10it/s, loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:23<00:00,  5.10it/s, loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:22<00:00,  5.10it/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:24<00:00,  5.09it/s, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:24<00:00,  5.09it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:27<00:00,  5.09it/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coco_prefix: 100%|████████████████████████████████████████████| 15410/15410 [50:27<00:00,  5.09it/s, loss=1.68]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from typing import Tuple, Optional, Union\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "class ClipCocoDataset(Dataset):\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        return tokens, mask, self.prefixes[self.caption2embedding[item]], self.captions[item]\n",
    "\n",
    "    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\"):\n",
    "        #self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                          bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                          pad_token='<pad>', mask_token='<mask>') \n",
    "        self.prefix_length = prefix_length\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        self.captions = [caption['caption'] for caption in captions_raw]\n",
    "        if os.path.isfile(\"/home/i15project23/data/coco/oscar_split_train_tokens.pkl\"):\n",
    "            with open(\"/home/i15project23/data/coco/oscar_split_train_tokens.pkl\", 'rb') as f:\n",
    "                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n",
    "        else:\n",
    "            self.captions_tokens = []\n",
    "            self.caption2embedding = []\n",
    "            max_seq_len = 0\n",
    "            for caption in captions_raw:\n",
    "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64))\n",
    "                self.caption2embedding.append(caption[\"clip_embedding\"])\n",
    "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "            # self.max_seq_len = max_seq_len\n",
    "            #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "            with open(\"/home/i15project23/data/coco/oscar_split_train_tokens.pkl\", 'wb') as f:\n",
    "                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
    "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
    "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                 self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "def save_config(args: argparse.Namespace):\n",
    "    config = {}\n",
    "    for key, item in args._get_kwargs():\n",
    "        config[key] = item\n",
    "    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n",
    "    with open(out_path, 'w') as outfile:\n",
    "        json.dump(config, outfile)\n",
    "\n",
    "\n",
    "def load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.set_defaults(**config)\n",
    "    args = parser.parse_args()\n",
    "    if type(epoch_or_latest) is int:\n",
    "        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n",
    "    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n",
    "    if args.only_prefix:\n",
    "        model = ClipCaptionPrefix(args.prefix_length)\n",
    "    else:\n",
    "        model = ClipCaptionModel(args.prefix_length)\n",
    "    if os.path.isfile(model_path):\n",
    "        print(f\"loading model from {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        print(f\"{model_path} is not exist\")\n",
    "    return model, parser\n",
    "\n",
    "\n",
    "def train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n",
    "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "    batch_size = args.bs\n",
    "    epochs = args.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "    save_config(args)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        for idx, (tokens, mask, prefix, _) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            progress.update()\n",
    "            '''\n",
    "            if (idx + 1) % 10000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "            '''\n",
    "        progress.close()\n",
    "        if epoch % args.save_every == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"/home/i15project23/data/coco/output_\" + str(epoch) + \".pt\",\n",
    "            )\n",
    "            #os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', default='/home/i15project23/data/coco/oscar_split_train.pkl')\n",
    "    parser.add_argument('--out_dir', default='./checkpoints')\n",
    "    parser.add_argument('--prefix', default='coco_prefix', help='prefix for saved filenames')\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--save_every', type=int, default=1)\n",
    "    parser.add_argument('--prefix_length', type=int, default=10)\n",
    "    parser.add_argument('--bs', type=int, default=40)\n",
    "    parser.add_argument('--only_prefix', dest='only_prefix', action='store_true')\n",
    "    parser.set_defaults(only_prefix=False)\n",
    "    args = parser.parse_args('')\n",
    "    prefix_length = args.prefix_length\n",
    "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
    "    if args.only_prefix:\n",
    "        model = ClipCaptionPrefix(prefix_length)\n",
    "        print(\"Train only prefix\")\n",
    "    else:\n",
    "        model = ClipCaptionModel(prefix_length)\n",
    "        print(\"Train both prefix and GPT\")\n",
    "        sys.stdout.flush()\n",
    "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T23:17:51.090487Z",
     "iopub.status.busy": "2021-11-23T23:17:51.090190Z",
     "iopub.status.idle": "2021-11-23T23:17:51.795958Z",
     "shell.execute_reply": "2021-11-23T23:17:51.794931Z",
     "shell.execute_reply.started": "2021-11-23T23:17:51.090453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oscar_split_train.pkl\t      output_1.pt  output_4.pt\toutput_7.pt\r\n",
      "oscar_split_train_tokens.pkl  output_2.pt  output_5.pt\toutput_8.pt\r\n",
      "output_0.pt\t\t      output_3.pt  output_6.pt\toutput_9.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/coco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T23:25:39.053885Z",
     "iopub.status.busy": "2021-11-23T23:25:39.053634Z",
     "iopub.status.idle": "2021-11-23T23:25:39.060142Z",
     "shell.execute_reply": "2021-11-23T23:25:39.059430Z",
     "shell.execute_reply.started": "2021-11-23T23:25:39.053855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/coco/output_9.pt' target='_blank'>data/coco/output_9.pt</a><br>"
      ],
      "text/plain": [
       "/home/i15project23/data/coco/output_9.pt"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(\"data/coco/output_9.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
